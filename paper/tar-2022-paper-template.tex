% Paper template for TAR 2022
% (C) 2014 Jan Šnajder, Goran Glavaš, Domagoj Alagić, Mladen Karan
% TakeLab, FER

\documentclass[10pt, a4paper]{article}

\usepackage{tar2022}

\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}

\title{Making your tweets more ironic}

\name{Kristian Djaković, Rene Kustura, Toma Puljak} 

\address{
University of Zagreb, Faculty of Electrical Engineering and Computing\\
Unska 3, 10000 Zagreb, Croatia\\ 
\texttt{\{author\}@fer.hr}\\
}
          
         
\abstract{ 
This document provides the instructions on formatting the TAR system description paper in \LaTeX{}. This is where you write the abstract (i.e., summary) of the work you carried out within the project. The abstract is a paragraph of text ranging between 70 and 150 words.
}

\begin{document}

\maketitleabstract

\section{Introduction}

Irony is complicated.
% Most people can probably tell if something is ironic or not when they see it.
% On the other hand, majority can't actually explain what it really is, and that's pretty ironic.
In general, irony refers to a clash between expectation and outcome of an expression. \citep{kreutz-20}. 
While this definition looks simple enough, detecting irony in text proves to be a difficult task, not only for models, but for people sometimes as well.
This paper tackles the question: "What makes tweets ironic?" by analysing attention values outputed from a classification model.

The attention mechanism allows modeling of dependencies without regard to their distance in the input or output sequence (CITE FROM ATTENTION IS ALL YOU NEED).
What's great about attention is that it adds a layer or explainability to the model. Attention values can be used to interpret the focus of a model when completing various tasks.
In this paper, attention values are used to gather most ironic tokens from data samples. Most ironic tokens being those with the highest attention values.

For classification purposes, we used a pretrained RoBERTa Transformer model that was trained on the SemEval2018 Irony Detection dataset (REFERENCE TO THE TRANSFORMER AND DATASET).
The model uses multi-head attention, thus we used the \textit{attention rollout} method to aggregate attention values across heads for single tokens and connections between token pairs.

Once we gathered the most ironic tokens, we applied them to tweets that the model classified as \textit{non ironic} and observed the change between the prediction certainty.
We use this change in prediction certainty as a metric to evaluate how much a token, or token pair, impacts the ironic sentiment of a given tweet.

\section{Background}

In this section, we provide an overview of the classification model setup, attention aggregation methods and the datasets evaluate our analysis goal.

\subsection{Model setup}
\label{sec:first}

For classification purposes, we used a pretrained RoBERTa Transformer model that had the best performance in irony detection.
The model can be found here: \url{https://huggingface.co/cardiffnlp/twitter-roberta-base-irony}.
RoBERTa is a retrained BERT model with improved performance. The details of the architecture can be found in (CITATION). 

\subsection{Attention aggregation}
\label{sec:attaggr}

To aggregate attention values into a singular output we use the \textit{attention rollout} method.
Attention rollout is a recursive method that calculates the product of all attention weights in the attention graph. (CITE)
This method will compute a singular output for a single token (node) that we feed into it and if we squeeze the resulting matrix without extraction, we get attention values for two connected tokens in the input sentence.

\subsection{Dataset}

The dataset used to train the RoBERTa model for irony detection is the SemEval2018 Irony Detection dataset (CITE DATASET) which is presplit into training and testing sets.
Each tweet is annotated with a binary label of 0 or 1, representing a non-ironic and ironic label respectively.

% This is the second subsection of the second section. Referencing the (sub)sections in text is performed as follows: ``in Section~\ref{sec:first} we have shown \dots''.

% \subsubsection{Sub-subsection example} 

% This is a sub-subsection. If possible, it is better to avoid sub-subsections. 

\section{Analysing the most ironic tokens}

In this section, we will cover the process of finding the most ironic tokens from the used dataset and the methods used for evaluating the results.

\subsection{Finding the tokens}

The first step in finding the tokens is to extract all tweets that were annotated as ironic from the training and testing datasets, keeping them separate so we can test the results of the experiment separately.
We then classified those tweets using the RoBERTa model to obtain a confidence score for each label.
Sorting the tweets regarding to their ironic label confidence score we obtained a list of the most ironic tweets in the datasets, accoring to the model.
A list of the most ironic tweets can be seen in (ADD TABLE 1).

Once we obtained the list of the most ironic tweets, we performed attention aggregation methods outlined in Section~\ref{sec:attaggr} and sorted the tokens by the attention score in a descending order.
That gives us a list of singular and two connected tokens that the model deemed most ironic for each of the most ironic tweets. 
The obtained list can be seen in (ADD TABLE 2).

\subsection{Adding the tokens to non ironic text}

Using a similar method to extracting the most ironic tweets, we obtained a list of tweets with the highest non ironic confidence score.
We used a subset of this list for analysing the impact of newly introduced ironic tokens. (DEFINE EXACTLY THE SUBSET)

To add the ironic tokens into the non-ironic text we simply appended them to the end of the tweet. This was done for each token in the list applied to each tweet in the subset.
We are aware that simply appending the token, or tokens, to the end of a tweet may generate something meaningless but ensuring that the newely formed tweet is semanticaly correct is out of the scope of this paper and we leave that for future work.

\section{Results}

\subsection{Single token}

We analysed (X) tweets paired with (Y) tokens. A sample of the top (N) and bottom (M) changes in non-ironic confidence scores can be seen in (ADD TABLE), with the full results available at: (LINK TO GITHUB).

The table show that the "most ironic" token was (X) and the "least ironic" was (Y). (ADD MORE COMMENTS ON THE RESULTS)

\subsection{Two tokens}

We analysed (X) tweets paired with (Y) pairs of tokens. A sample of the top (N) and bottom (M) changes in non-ironic confidence scores can be seen in (ADD TABLE), with the full results available at: (LINK TO GITHUB).

The table show that the "most ironic" pair of tokens was (X) and the "least ironic" was (Y). (ADD MORE COMMENTS ON THE RESULTS)

\section{Conclusion}


\section{Figures and tables}

\subsection{Figures}

Here is an example on how to include figures in the paper. Figures are included in \LaTeX{} code immediately \textit{after} the text in which these figures are referenced. Allow \LaTeX{} to place the figure where it believes is best (usually on top of the page of at the position where you would not place the figure). Figures are referenced as follows: ``Figure~\ref{fig:figure1} shows \dots''. Use tilde (\verb.~.) to prevent separation between the word ``Figure'' and its enumeration. 

\begin{figure}
\begin{center}
\includegraphics[width=\columnwidth]{drawing.pdf}
\caption{This is the figure caption. Full sentences should be followed with a dot. The caption should be placed \textit{below} the figure. Caption should be short; details should be explained in the text.}
\label{fig:figure1}
\end{center}
\end{figure}

\subsection{Tables}

There are two types of tables: narrow tables that fit into one column and a wide table that spreads over both columns.

\subsubsection{Narrow tables}

Table~\ref{tab:narrow-table} is an example of a narrow table. Do not use vertical lines in tables -- vertical tables have no effect and they make tables visually less attractive. We recommend using \textit{booktabs} package for nicer tables.

\begin{table}
\caption{This is the caption of the table. Table captions should be placed \textit{above} the table.}
\label{tab:narrow-table}
\begin{center}
\begin{tabular}{ll}
\toprule
Heading1 & Heading2 \\
\midrule
One & First row text \\
Two   & Second row text \\
Three   & Third row text \\
      & Fourth row text \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Wide tables}

Table~\ref{tab:wide-table} is an example of a wide table that spreads across both columns. The same can be done for wide figures that should spread across the whole width of the page. 

\begin{table*}
\caption{Wide-table caption}
\label{tab:wide-table}
\begin{center}
\begin{tabular}{llr}
\toprule
Heading1 & Heading2 & Heading3\\
\midrule
A & A very long text, longer that the width of a single column & $128$\\
B & A very long text, longer that the width of a single column & $3123$\\
C & A very long text, longer that the width of a single column & $-32$\\
\bottomrule
\end{tabular}
\end{center}
\end{table*}

\section{Math expressions and formulas}

Math expressions and formulas that appear within the sentence should be written inside the so-called \emph{inline} math environment: $2+3$, $\sqrt{16}$, $h(x)=\mathbf{1}(\theta_1 x_1 + \theta_0>0)$. Larger expressions and formulas (e.g., equations) should be written in the so-called \emph{displayed} math environment:

\[
b^{(i)}_k = \begin{cases}
1 & \text{if 
    $k = \text{argmin}_j \| \mathbf{x}^{(i)} - \mathbf{\mu}_j \|,$}\\
0 & \text{otherwise}
\end{cases}
\]

Math expressions which you reference in the text should be written inside the \textit{equation} environment:

\begin{equation}\label{eq:kmeans-error}
J = \sum_{i=1}^N \sum_{k=1}^K 
b^{(i)}_k \| \mathbf{x}^{(i)} - \mathbf{\mu}_k \|^2
\end{equation}

Now you can reference equation \eqref{eq:kmeans-error}. If the paragraph continues right after the formula

\begin{equation}
f(x) = x^2 + \varepsilon
\end{equation}

\noindent like this one does, use the command \emph{noindent} after the equation to remove the indentation of the row. 

Multi-letter words in the math environment should be written inside the command \emph{mathit}, otherwise \LaTeX{} will insert spacing between the letters to denote the multiplication of values denoted by symbols. For example, compare
$\mathit{Consistent}(h,\mathcal{D})$ and\\
$Consistent(h,\mathcal{D})$.

If you need a math symbol, but you don't know the corresponding \LaTeX{} command that generates it, try
\emph{Detexify}.\footnote{\texttt{http://detexify.kirelabs.org/}}

\section{Referencing literature}

References to other publications should be written in brackets with the last name of the first author and the year of publication, e.g., \citep{chomsky-73}.  Multiple references are written in sequence, one after another, separated by semicolon and without whitespaces in between, e.g., \citep{chomsky-73,chave-64,feigl-58}. References are typically written at the end of the sentence and necessarily before the sentence punctuation.

If the publication is authored by more than one author, only the name of the first author is written, after which abbreviation \emph{et al.}, meaning \emph{et alia}, i.e.,~and others is written as in \citep{johnson-etc}. If the publication is authored by only two authors, then the last names of both authors are written \citep{johnson-howells}.

If the name of the author is incorporated into the text of the sentence, it should not be in the brackets (only the year should be there). E.g.,~``\citet{chomsky-73}
suggested that \dots''. The difference is whether you reference the publication or the author who wrote it. 

The list of all literature references is given alphabetically at the end of the paper. The form of the reference depends on the type of the bibliographic unit: conference papers,
\citep{chave-64}, books \citep{butcher-81}, journal articles
\citep{howells-51}, doctoral dissertations \citep{croft-78}, and book chapters \citep{feigl-58}. 

All of this is automatically produced when using BibTeX. Insert all the BibTeX entries into the file \texttt{tar2022.bib}, and then reference them via their symbolic names.

\section{Conclusion}

Conclusion is the last enumerated section of the paper. It should not exceed half of a column and is typically split into 2--3 paragraphs. No new information should be presented in the conclusion; this section only summarizes and concludes the paper.

\section*{Acknowledgements}

If suitable, you can include the \textit{Acknowledgements} section before inserting the literature references  in order to thank those who helped you in any way to deliver the paper, but are not co-authors of the paper.

\bibliographystyle{tar2022}
\bibliography{tar2022} 

\end{document}

